{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamining Twitter with tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least 7 python interfaces to the Twitter WEB Application Programming Interface (API).  We will use `tweepy`, since the [documentation is clear](http://www.tweepy.org/), and there are [interesting applications available to get started](http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will need to install tweepy.  The most straightforward way is through the `pip` installation tool.  This can be run from the command line using:\n",
    "\n",
    "    pip install tweepy\n",
    "    \n",
    "or from within a Canopy IPython shell:\n",
    "\n",
    "    %bash pip install tweepy\n",
    "    \n",
    "If you get this Exception:\n",
    "\n",
    "    TypeError: parse_requirements() got an unexpected keyword argument 'session'\n",
    "\n",
    "\n",
    "Make sure you upgrade pip to the newest version:\n",
    "\n",
    "    pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter uses the [OAuth protocol](https://dev.twitter.com/oauth/overview/faq) for secure application development.  Considering all of the applications that access Twitter (for example, using your Twitter account to login to a different website), this protocol prevents information like your password being passed through these intermediate accounts.  While this is a great security measure for intermediate client access, it adds an extra step for us before we can directly communicate with the API.  To access Twitter, you need to Create an App (https://apps.twitter.com); however, I've already created an app that we can all ping from: `GWU_TEST_APP`.   To interact with `GWU_TEST_APP`, you'll need an access token.  \n",
    "\n",
    "<br>\n",
    "\n",
    "[Request an access token here.](https://apps.twitter.com/app/7965526/keys)\n",
    "\n",
    "<br>\n",
    "\n",
    "Store your consumer key and comumer secret somewhere you'll remember them.  I'm storing mine in Python strings, but for security, not displaying this step:\n",
    "\n",
    "    consumer_key = 'jrCYD....'\n",
    "    consumer_secret = '...' \n",
    "    \n",
    "\n",
    "Here is a discussion on the difference between the access token and the consumer token; although, for our intents and purposes, its not so important: http://stackoverflow.com/questions/20720752/whats-the-difference-between-twitter-consumer-key-and-access-token**\n",
    "\n",
    "```\n",
    "The consumer key is for your application and client tokens are for end users in your application's context. If you want to call in just the application context, then consumer key is adequate. You'd be rate limited per application and won't be able to access user data that is not public. With the user token context, you'll be rate limited per token/user, this is desirable if you have several users and need to make more calls than application context rate limiting allows. This way you can access private user data. Which to use depends on your scenarios.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Read Tweets Appearing on Homepage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `consumer_key` and `consumer_secret` stored, let's try a Hello World example from Tweepy's docs.  This will access the public tweets appearing on the User's feed as if they had logged in to twitter.  **For brevity, we'll only print the first two**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET 0:\n",
      "\n",
      "\tRT @Gawker: AccuWeather slams the NWS for missing a tornado AccuWeather didn't cover. http://t.co/2NUp31vJHa\n",
      "\n",
      "\n",
      "TWEET 1:\n",
      "\n",
      "\tRT @SarcasticRover: This comic isn't about me, and yet, I still like it: http://t.co/ojdydJDDSG http://t.co/glVj7hRsM9\n",
      "\n",
      "\n",
      "TWEET 2:\n",
      "\n",
      "\tSad news tonight: No Hays eaglets in 2015 http://t.co/m2jULOTPCk http://t.co/pjnZsCRdxm\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = 'jrCYD9dREozKRfchtkm6zg02Z'\n",
    "consumer_secret = 'h0cWbg5TeV2AS1n5w33ZwALEQcS4JkC2rpOXNfIImOHL8hdFLg'\n",
    "\n",
    "access_token ='718576069-CGK0f03Q94CkFysA6OJgJZeRBef2AGIh1bzceVl4'\n",
    "access_token_secret = 'zdOaZWEncust1rFGKAWaj462VRUD6GMcU60plkCaobfEf'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for (idx, tweet) in enumerate(public_tweets[0:3]): #First 3 tweets in my public feed\n",
    "    print 'TWEET %s:\\n\\n\\t%s\\n\\n' % (idx, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used `tweet.text`, we implicitly used a python class defined by `tweepy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.Status"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many attributes associated with a `Status` object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'truncated',\n",
       " 'text',\n",
       " 'in_reply_to_status_id',\n",
       " 'id',\n",
       " 'favorite_count',\n",
       " '_api',\n",
       " 'author',\n",
       " '_json',\n",
       " 'coordinates',\n",
       " 'entities',\n",
       " 'in_reply_to_screen_name',\n",
       " 'id_str',\n",
       " 'retweet_count',\n",
       " 'in_reply_to_user_id',\n",
       " 'favorited',\n",
       " 'source_url',\n",
       " 'user',\n",
       " 'geo',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'possibly_sensitive',\n",
       " 'possibly_sensitive_appealable',\n",
       " 'lang',\n",
       " 'created_at',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'place',\n",
       " 'source',\n",
       " 'extended_entities',\n",
       " 'retweeted']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: What's trending where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [tweepy API](http://tweepy.readthedocs.org/en/v3.2.0/api.html), we can return the top 10 trending topics for a specific location, where the location is a `WOEID (Yahoo Where on Earth ID)`. \n",
    "\n",
    "<br>\n",
    "\n",
    "The WOEID is a unique identifier, similar to zipcodes, but that expand worldwide.  For example, my hometown of Pittsburgh has a WOEID of 2473224.  You can search for WOEID's here: http://woeid.rosselliot.co.nz/\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's return the top ten trending topics in Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'as_of': u'2015-03-28T00:59:24Z',\n",
       "  u'created_at': u'2015-03-28T00:56:51Z',\n",
       "  u'locations': [{u'name': u'Pittsburgh', u'woeid': 2473224}],\n",
       "  u'trends': [{u'name': u'#CallMeCam',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'%23CallMeCam',\n",
       "    u'url': u'http://twitter.com/search?q=%23CallMeCam'},\n",
       "   {u'name': u'#WVUvsUK',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'%23WVUvsUK',\n",
       "    u'url': u'http://twitter.com/search?q=%23WVUvsUK'},\n",
       "   {u'name': u'#CarrotForANight',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'%23CarrotForANight',\n",
       "    u'url': u'http://twitter.com/search?q=%23CarrotForANight'},\n",
       "   {u'name': u'Ultra',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'Ultra',\n",
       "    u'url': u'http://twitter.com/search?q=Ultra'},\n",
       "   {u'name': u'Huggins',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'Huggins',\n",
       "    u'url': u'http://twitter.com/search?q=Huggins'},\n",
       "   {u'name': u'Pittsburgh',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'Pittsburgh',\n",
       "    u'url': u'http://twitter.com/search?q=Pittsburgh'},\n",
       "   {u'name': u'Notre Dame',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'%22Notre+Dame%22',\n",
       "    u'url': u'http://twitter.com/search?q=%22Notre+Dame%22'},\n",
       "   {u'name': u'UCLA',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'UCLA',\n",
       "    u'url': u'http://twitter.com/search?q=UCLA'},\n",
       "   {u'name': u'Trailer Park Boys',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'%22Trailer+Park+Boys%22',\n",
       "    u'url': u'http://twitter.com/search?q=%22Trailer+Park+Boys%22'},\n",
       "   {u'name': u'Pens',\n",
       "    u'promoted_content': None,\n",
       "    u'query': u'Pens',\n",
       "    u'url': u'http://twitter.com/search?q=Pens'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = api.trends_place(id=2473224)\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `JSON` object.  JSON is a human and machine-readable standardized data encoding format.  \n",
    "\n",
    "<br>\n",
    "\n",
    "In Python, JSON objects are implemented as lists of nested dictionaries.  JSON stands for JavaScript Object Notation, because it's designed based on a subset of the JavaScript language; however, JSON is a data-encoding format implemented in many languages.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Looking at this structure, we see that it's contained in a list; in fact its a list of one element.  Let's access the top ten tweet names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'name': u'#CallMeCam',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'%23CallMeCam',\n",
       "  u'url': u'http://twitter.com/search?q=%23CallMeCam'},\n",
       " {u'name': u'#WVUvsUK',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'%23WVUvsUK',\n",
       "  u'url': u'http://twitter.com/search?q=%23WVUvsUK'},\n",
       " {u'name': u'#CarrotForANight',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'%23CarrotForANight',\n",
       "  u'url': u'http://twitter.com/search?q=%23CarrotForANight'},\n",
       " {u'name': u'Ultra',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'Ultra',\n",
       "  u'url': u'http://twitter.com/search?q=Ultra'},\n",
       " {u'name': u'Huggins',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'Huggins',\n",
       "  u'url': u'http://twitter.com/search?q=Huggins'},\n",
       " {u'name': u'Pittsburgh',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'Pittsburgh',\n",
       "  u'url': u'http://twitter.com/search?q=Pittsburgh'},\n",
       " {u'name': u'Notre Dame',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'%22Notre+Dame%22',\n",
       "  u'url': u'http://twitter.com/search?q=%22Notre+Dame%22'},\n",
       " {u'name': u'UCLA',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'UCLA',\n",
       "  u'url': u'http://twitter.com/search?q=UCLA'},\n",
       " {u'name': u'Trailer Park Boys',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'%22Trailer+Park+Boys%22',\n",
       "  u'url': u'http://twitter.com/search?q=%22Trailer+Park+Boys%22'},\n",
       " {u'name': u'Pens',\n",
       "  u'promoted_content': None,\n",
       "  u'query': u'Pens',\n",
       "  u'url': u'http://twitter.com/search?q=Pens'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10[0]['trends']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's alot of metadata that goes into even a simple tweet.  Let's cycle through each of these trends, and print the `name` and website of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CallMeCam http://twitter.com/search?q=%23CallMeCam\n",
      "#WVUvsUK http://twitter.com/search?q=%23WVUvsUK\n",
      "#CarrotForANight http://twitter.com/search?q=%23CarrotForANight\n",
      "Ultra http://twitter.com/search?q=Ultra\n",
      "Huggins http://twitter.com/search?q=Huggins\n",
      "Pittsburgh http://twitter.com/search?q=Pittsburgh\n",
      "Notre Dame http://twitter.com/search?q=%22Notre+Dame%22\n",
      "UCLA http://twitter.com/search?q=UCLA\n",
      "Trailer Park Boys http://twitter.com/search?q=%22Trailer+Park+Boys%22\n",
      "Pens http://twitter.com/search?q=Pens\n"
     ]
    }
   ],
   "source": [
    "for trend in top10[0]['trends']:\n",
    "    print trend['name'], trend['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Streaming and Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This Streaming tutorial follows closely [Adil Moujahid's great tweepy examples](http://adilmoujahid.com/posts/2014/07/twitter-analytics/)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Twitter offers a [Streaming API](https://dev.twitter.com/streaming/overview) to make it easier to query streams of tweets.  The Stream API encapsulates some pain points of REST access to ensure that Stream calls don't exceed the rate limit.  Think of them as Twitter's suggested means to stream data for beginners.  You don't have to use them, but they're recommended and will make life easier.  There are three stream types:\n",
    "\n",
    "   - `Public Streams:` Streams of public data flowthing through Twitter.  Suitable for followign specific users, topics or for data mining.\n",
    "    \n",
    "   - `User Streams:` Single-user streams.  Containing roughly all of the data corresponding with a single user's view of Twitter.\n",
    "    \n",
    "   - `Site Streams:`  The multi-user version of user streams.  \n",
    "   \n",
    "<br>\n",
    "    \n",
    "We'll resist the temptation to mess with our friend's Twitter accounts, and focus soley on `Public Streams`.  Combining these stream with text filters will let us accumulate content.  For example, we could look for tweets involving the text, *foxnews*.  `tweepy` and `Twitter's API` will configure the stream and filter to work nicely, you just provide the content tags you're interested in.  Finally, **remember that the more obsucre the content, the longer it will take to find**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**<font color='red'>The following snippet will run until `max_tweets` or `max_seconds` is reached.  If running in notebook, it will hold up cells until the alotted time.  Therefore, for long runtimes, you may want to run in an external python program, and then can terminate at will if desired.  I also recommend restarting notebook kernal before running this cell multiple times...</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 at 2015-03-27 21:36:23.312925.\n",
      "Ellapsed: 0.82 seconds\n",
      "\n",
      "Tweet 2 at 2015-03-27 21:36:23.398453.\n",
      "Ellapsed: 0.91 seconds\n",
      "\n",
      "Tweet 3 at 2015-03-27 21:36:24.039299.\n",
      "Ellapsed: 1.55 seconds\n",
      "\n",
      "Tweet 4 at 2015-03-27 21:36:26.303319.\n",
      "Ellapsed: 3.81 seconds\n",
      "\n",
      "Tweet 5 at 2015-03-27 21:36:28.103006.\n",
      "Ellapsed: 5.61 seconds\n",
      "\n",
      "Tweet 6 at 2015-03-27 21:36:32.845835.\n",
      "Ellapsed: 10.36 seconds\n",
      "\n",
      "Tweet 7 at 2015-03-27 21:36:34.851696.\n",
      "Ellapsed: 12.36 seconds\n",
      "\n",
      "Tweet 8 at 2015-03-27 21:36:35.339390.\n",
      "Ellapsed: 12.85 seconds\n",
      "\n",
      "Tweet 9 at 2015-03-27 21:36:35.912185.\n",
      "Ellapsed: 13.42 seconds\n",
      "\n",
      "Tweet 10 at 2015-03-27 21:36:36.107684.\n",
      "Ellapsed: 13.62 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Max tweets of 10 exceeded.  Killing stream... see <closed file 'test.txt', mode 'w' at 0x7f0e00a57f60>",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Max tweets of 10 exceeded.  Killing stream... see <closed file 'test.txt', mode 'w' at 0x7f0e00a57f60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StreamParser(StreamListener):\n",
    "    \"\"\" Controls how streaming data is parsed. Pass an outfile, or data will be writting to \n",
    "    sys.stdout (eg the screen)\n",
    "    \"\"\"\n",
    "    def __init__(self, outfile=None, max_tweets=5, max_seconds=30):\n",
    "        self.counter = 0\n",
    "        self.start_time = time.time()\n",
    "        # Set upper limits on maximum tweets or seconds before timeout\n",
    "        self.max_tweets = max_tweets\n",
    "        self.max_seconds = max_seconds\n",
    "        if outfile:\n",
    "            self.stdout = open(outfile, 'w')\n",
    "        else:\n",
    "            self.stdout = sys.stdout\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        \"\"\" Data is a string, but formatted for json. Parses it\"\"\"\n",
    "        self.counter += 1\n",
    "        # time data is all timestamps.\n",
    "        current_time = time.time()\n",
    "        run_time = current_time - self.start_time\n",
    "                \n",
    "        # If we want to read time, easiest way is to convert from timestamp using datetime\n",
    "        formatted_time = datetime.datetime.now()\n",
    "            \n",
    "        # Technically, might not be the best place to put kill statements, but works well enough\n",
    "        if self.max_tweets:\n",
    "            if self.counter > self.max_tweets:\n",
    "                self._kill_stdout()\n",
    "                raise SystemExit('Max tweets of %s exceeded.  Killing stream... see %s' \\\n",
    "                             % (self.max_tweets, self.stdout))\n",
    "  \n",
    "        if self.max_seconds:\n",
    "            if run_time > self.max_seconds:\n",
    "                self._kill_stdout()\n",
    "                raise SystemExit('Max time of %s seconds exceeded.  Killing stream... see %s' \\\n",
    "                                 % (self.max_seconds, self.stdout))\n",
    "\n",
    "        print 'Tweet %s at %s.\\nEllapsed: %.2f seconds\\n' % \\\n",
    "             (self.counter, formatted_time, run_time)\n",
    "\n",
    "        # Write to file, return True causes stream to continue I guess...\n",
    "        self.stdout.write(data)\n",
    "        return True\n",
    "\n",
    "    def _kill_stdout(self):\n",
    "        \"\"\" If self.stdout is a file, close it.  If sys.stdout, pass\"\"\"\n",
    "        if self.stdout is not sys.stdout:\n",
    "            self.stdout.close() \n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Stream 10 tweets, no matter the time it takes!\n",
    "listener = StreamParser(outfile='test.txt', max_tweets=10, max_seconds=None)\n",
    "stream = Stream(auth, listener)\n",
    "\n",
    "#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "stream.filter(track=['ipython', 'drugs', 'rockandroll'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only one tweet were saved, we could just use json.loads() to read it in right away, but\n",
    "for a file with multiple tweets, we need to [read them in one at a time](http://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data). \n",
    "\n",
    "<br>\n",
    "\n",
    "Each tweet JSON object is one long line, so we can read in line by line, until an error is reached in which case we just stop.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('test.txt', 'r'):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet text itself is embedded in the `text` metadata field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'@_youhadonejob Also took the record amount of drugs. Unofficially.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out all of the metadata you can get from a tweet! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'contributors',\n",
       " u'coordinates',\n",
       " u'created_at',\n",
       " u'entities',\n",
       " u'favorite_count',\n",
       " u'favorited',\n",
       " u'filter_level',\n",
       " u'geo',\n",
       " u'id',\n",
       " u'id_str',\n",
       " u'in_reply_to_screen_name',\n",
       " u'in_reply_to_status_id',\n",
       " u'in_reply_to_status_id_str',\n",
       " u'in_reply_to_user_id',\n",
       " u'in_reply_to_user_id_str',\n",
       " u'lang',\n",
       " u'place',\n",
       " u'possibly_sensitive',\n",
       " u'retweet_count',\n",
       " u'retweeted',\n",
       " u'source',\n",
       " u'text',\n",
       " u'timestamp_ms',\n",
       " u'truncated',\n",
       " u'user']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tweets[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within these fields, there's even more information.  For example, the `user` and `entities` fields, which provide information about the `user` as well as links and images (entities) embedded in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'contributors_enabled': False,\n",
       " u'created_at': u'Wed Jan 11 13:44:33 +0000 2012',\n",
       " u'default_profile': True,\n",
       " u'default_profile_image': False,\n",
       " u'description': u'Football lover, sun worshipper, accounts wizard, pool hustler, stargazer, runner.',\n",
       " u'favourites_count': 155,\n",
       " u'follow_request_sent': None,\n",
       " u'followers_count': 74,\n",
       " u'following': None,\n",
       " u'friends_count': 186,\n",
       " u'geo_enabled': False,\n",
       " u'id': 461128587,\n",
       " u'id_str': u'461128587',\n",
       " u'is_translator': False,\n",
       " u'lang': u'en',\n",
       " u'listed_count': 1,\n",
       " u'location': u'Eastbourne',\n",
       " u'name': u'Keith Axell',\n",
       " u'notifications': None,\n",
       " u'profile_background_color': u'C0DEED',\n",
       " u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       " u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       " u'profile_background_tile': False,\n",
       " u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/461128587/1384693366',\n",
       " u'profile_image_url': u'http://pbs.twimg.com/profile_images/439538775002456064/GiQUGj7E_normal.jpeg',\n",
       " u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/439538775002456064/GiQUGj7E_normal.jpeg',\n",
       " u'profile_link_color': u'0084B4',\n",
       " u'profile_sidebar_border_color': u'C0DEED',\n",
       " u'profile_sidebar_fill_color': u'DDEEF6',\n",
       " u'profile_text_color': u'333333',\n",
       " u'profile_use_background_image': True,\n",
       " u'protected': False,\n",
       " u'screen_name': u'Axellmania',\n",
       " u'statuses_count': 365,\n",
       " u'time_zone': u'Casablanca',\n",
       " u'url': None,\n",
       " u'utc_offset': 0,\n",
       " u'verified': False}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'hashtags': [],\n",
       " u'symbols': [],\n",
       " u'trends': [],\n",
       " u'urls': [],\n",
       " u'user_mentions': [{u'id': 1653442136,\n",
       "   u'id_str': u'1653442136',\n",
       "   u'indices': [0, 14],\n",
       "   u'name': u'You had one job',\n",
       "   u'screen_name': u'_youhadonejob'}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Alchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Alchemy API is an artificial intelligence toolkit for machine learning needs like facial recognition, sentiment analysis and so forth.  IIUC it's built in some part over scikit-learn, and has python roots.  With a Python SDK, it's a great opportunity to do some bigboy analysis of these twitter streams.\n",
    "\n",
    "http://www.alchemyapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/glue/Desktop/alchemyapi_python/')\n",
    "from alchemyapi import AlchemyAPI as alcapi\n",
    "from types import MethodType\n",
    "\n",
    "ALCAPI = alcapi() #<-- Instantiate\n",
    "print 'The available attributes and methods for Alchemy API are:\\n' \n",
    "sorted(alcapi.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(alcapi.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOXARTICLE = 'http://www.foxnews.com/us/2015/02/24/southern-california-commuter-train-crashes-into-truck-injuries-reported/'\n",
    "GOODARTICLE = 'http://www.goodnewsnetwork.org/company-gives-employees-1000-job-well-done/'\n",
    "\n",
    "badnews = ALCAPI.sentiment('url', FOXARTICLE)\n",
    "goodnews = ALCAPI.sentiment('url', GOODARTICLE)\n",
    "\n",
    "print 'Article from fox news:\\n\\t', badnews['docSentiment']\n",
    "print '\\n'\n",
    "print 'Article from goodnews news:\\n\\t', goodnews['docSentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "image_extract = ALCAPI.imageExtraction('url', GOODARTICLE)\n",
    "\n",
    "# Use ipython's display system to render the image\n",
    "Image(image_extract['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it found an ad on the page, not the actual main image.  Let's try the \"always infer\" option which is supposed to be more rigorous in getting algorithms (although I don't know how):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_extract = ALCAPI.imageExtraction('url', GOODARTICLE, options=dict(extractMode='always-infer'))\n",
    "\n",
    "# Use ipython's display system to render the image\n",
    "Image(image_extract['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an add appearing at the bottom of the page!  The actual image we want is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image('http://www.goodnewsnetwork.org/wp-content/uploads/2015/02/Joseph-Beyer-giant-check-for-1000-submitted.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged = ALCAPI.faceTagging('url',\n",
    "                   'http://www.goodnewsnetwork.org/wp-content/uploads/2015/02/Joseph-Beyer-giant-check-for-1000-submitted.jpg')\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this gives an X and Y position?  We can extract this using `scikit image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io as skio\n",
    "%pylab inline\n",
    "\n",
    "# Read into a skimage image\n",
    "somedude = skio.imread('http://www.goodnewsnetwork.org/wp-content/uploads/2015/02/Joseph-Beyer-giant-check-for-1000-submitted.jpg')\n",
    "imshow(somedude);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _parseFace(attr):\n",
    "    \"\"\" Shortcut for tagged['imageFaces'][0]['attr'] \"\"\"\n",
    "    return int(tagged['imageFaces'][0][attr])\n",
    "    \n",
    "X, Y, WIDTH, HEIGHT = _parseFace('positionX'), _parseFace('positionY'), _parseFace('width'), _parseFace('height')\n",
    "\n",
    "# Scikit image is reversed X, Y coordinates relative to these\n",
    "imshow(somedude[Y:Y+HEIGHT, X:X+WIDTH]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TWOPEEPS = 'http://media.northlandsnewscenter.com/images/400*264/tree_theft.jpg'\n",
    "imshow(skio.imread(TWOPEEPS));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is detected from this image from AlchemyAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twopeeps = ALCAPI.faceTagging('url', TWOPEEPS)\n",
    "twopeeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TREE = 'http://higherperspective.com/wp-content/uploads/2014/08/oak-tree.jpg'\n",
    "treepeeps = ALCAPI.faceTagging('url', TREE)\n",
    "\n",
    "imshow(skio.imread(TREE))\n",
    "treepeeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shit is pretty legit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code below changes notebook formatting/style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "div.out_prompt_overlay:hover{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_prompt:hover{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "@media (min-width: 550px) {\n",
       "  h1 { font-size: 5.0rem !important; }\n",
       "  h2 { font-size: 4.2rem !important; }\n",
       "  h3 { font-size: 3.6rem !important; }\n",
       "  h4 { font-size: 3.0rem !important; }\n",
       "  h5 { font-size: 2.4rem !important; }\n",
       "  h6 { font-size: 1.5rem !important; }\n",
       "}\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "  \n",
       "code {\n",
       "  padding: .2rem .5rem !important;\n",
       "  margin: 0 .2rem !important;\n",
       "  font-size: 90% !important;\n",
       "  white-space: nowrap !important;\n",
       "  background: #F1F1F1 !important;\n",
       "  border: 1px solid #E1E1E1 !important;\n",
       "  border-radius: 4px !important; }\n",
       "pre > code {\n",
       "  display: block !important;\n",
       "  padding: 1rem 1.5rem !important;\n",
       "  white-space: pre !important; }\n",
       "  \n",
       "button{ border-radius: 0px !important; }\n",
       ".navbar-inner{ background-image: none !important;  }\n",
       "select, textarea{ border-radius: 0px !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import urllib2\n",
    "HTML(urllib2.urlopen('http://bit.ly/1Bf5Hft').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 2)",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "signature": "sha256:fbf7851fba6c71cc62f88e2ead78363782be8638820adb9e4d42b9a28363c12f"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}